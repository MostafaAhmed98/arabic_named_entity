{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Explanation\n",
    "\n",
    "This code snippet utilizes the Hugging Face Datasets library to load and explore a Named Entity Recognition (NER) dataset for Arabic text. Here's a breakdown of each line:\n",
    "\n",
    "1. **Import Library:**\n",
    "   - `from datasets import load_dataset`: This line imports the `load_dataset` function from the `datasets` library. This function is used to load datasets from the Hugging Face Hub.\n",
    "\n",
    "2. **Load Dataset:**\n",
    "   - `raw_datasets = load_dataset(\"e-hossam96/conllpp-ner-ar\")`: This line loads the Arabic NER dataset named \"conllpp-ner-ar\" created by user \"e-hossam96\" from the Hugging Face Hub. The loaded dataset is stored in the `raw_datasets` variable.\n",
    "\n",
    "3. **Access Sample Data:**\n",
    "   - `raw_datasets[\"train\"][7][\"tokens\"], raw_datasets[\"train\"][7][\"ner_tags\"]`: These lines access specific information from the loaded dataset. They retrieve the following elements from the 8th sample (index 7) in the \"train\" split of the dataset:\n",
    "      - `raw_datasets[\"train\"][7][\"tokens\"]`: This extracts the list of tokens (words) from the sample.\n",
    "      - `raw_datasets[\"train\"][7][\"ner_tags\"]`: This extracts the list of NER tags corresponding to each token, indicating the named entity type (e.g., Person, Location).\n",
    "\n",
    "4. **Get Feature Information:**\n",
    "   - `ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]`: This line retrieves the feature definition for the \"ner_tags\" column from the \"train\" split. This feature definition describes the format and content of the NER tags.\n",
    "\n",
    "5. **Extract Label Names:**\n",
    "   - `label_names = ner_feature.feature.names`: This line extracts the list of possible NER tag names (e.g., \"B-PER\", \"I-LOC\") from the feature definition. These names represent the different entity types the model can identify.\n",
    "\n",
    "6. **Print Output:**\n",
    "   - `label_names, ner_feature`: This line likely represents the output of the code cell in a Jupyter Notebook. It displays both the extracted label names and the complete feature definition for the \"ner_tags\" column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"e-hossam96/conllpp-ner-ar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 10250\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 2383\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 2572\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['وقال',\n",
       "  'إن',\n",
       "  'الاقتراح',\n",
       "  'الذي',\n",
       "  'قدمه',\n",
       "  'الشهر',\n",
       "  'الماضي',\n",
       "  'مفوض',\n",
       "  'المزرعة',\n",
       "  'الاتحاد',\n",
       "  'الأوروبي',\n",
       "  'فرانز',\n",
       "  'فيشلر',\n",
       "  'بحظر',\n",
       "  'أدمغة',\n",
       "  'الأغنام',\n",
       "  'والطحال',\n",
       "  'والنخاع',\n",
       "  'الشوكي',\n",
       "  'من',\n",
       "  'السلسلة',\n",
       "  'الغذائية',\n",
       "  'البشرية',\n",
       "  'والحيوانية',\n",
       "  'كان',\n",
       "  'بمثابة',\n",
       "  'خطوة',\n",
       "  'احترازية',\n",
       "  'ومحددة',\n",
       "  'للغاية',\n",
       "  'لحماية',\n",
       "  'صحة',\n",
       "  'الإنسان.'],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][7][\"tokens\"], raw_datasets[\"train\"][7][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['O',\n",
       "  'B-PER',\n",
       "  'I-PER',\n",
       "  'B-ORG',\n",
       "  'I-ORG',\n",
       "  'B-LOC',\n",
       "  'I-LOC',\n",
       "  'B-MISC',\n",
       "  'I-MISC'],\n",
       " Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "label_names = ner_feature.feature.names\n",
    "label_names, ner_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الاتحاد الأوروبي يرفض الدعوة الألمانية لمقاطعة لحم الضأن البريطاني . \n",
      "B-ORG   I-ORG    O    O      B-MISC    O       O   O     B-MISC    O \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and implementing the alignment function\n",
    "\n",
    "Define a function `align_labels_with_tokens` that aligns NER tags with tokens after Arabic BERT-based tokenization.\n",
    "\n",
    "- **Imports:** `AutoTokenizer` from `transformers` for tokenization.\n",
    "- **Tokenizer:** Loads tokenizer for the Arabic BERT model (`aubmindlab/bert-base-arabertv02`).\n",
    "- **Function:** `align_labels_with_tokens` takes NER tags (`labels`) and token IDs (`word_ids`).\n",
    "- **Alignment:** Iterates through `word_ids`:\n",
    "    - Start of new word (different `word_id`): Gets corresponding label from `labels`.\n",
    "    - Special token ( `word_id` is None): Adds placeholder label (-100).\n",
    "    - Same word as previous: Gets label, converts Begin tags (odd values) to Inside tags for consistency.\n",
    "- **Returns:** The aligned NER tags (`new_labels`) matching the tokenization.\n",
    "\n",
    "This function ensures each token has a corresponding NER label after tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/anaconda3/envs/huggingface/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"aubmindlab/bert-base-arabertv02\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code preprocesses a dataset for Arabic NER with a transformer model:\n",
    "\n",
    "1. **Function:** `tokenize_and_align_labels` tokenizes text and aligns labels with tokens.\n",
    "2. **Tokenization:** Uses tokenizer to convert text to tokens.\n",
    "3. **Label Alignment:** Aligns original labels with tokens using `align_labels_with_tokens`.\n",
    "4. **Batch Processing:** Applies `tokenize_and_align_labels` to the entire dataset in batches.\n",
    "5. **Data Collator:** Creates a data collator for token classification tasks.\n",
    "6. **Batch Creation:** Creates a batch by selecting and padding examples for training.\n",
    "\n",
    "This prepares the data for training a transformer-based NER model on Arabic text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    2,   948,  2934,  5999,  4508,  4205, 37995, 12786,   792,   460,\n",
       "          4704,    20,     3,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " tensor([-100,    3,    4,    0,    0,    7,    0,    0,    0,    0,    7,    0,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(10)])\n",
    "batch['input_ids'][0],batch['labels'][0],batch['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing evaluate library to use it in model evaluation\n",
    "import evaluate\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'I-ORG', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MISC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'overall_precision': 0.6666666666666666,\n",
       " 'overall_recall': 0.6666666666666666,\n",
       " 'overall_f1': 0.6666666666666666,\n",
       " 'overall_accuracy': 0.9}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = labels.copy()\n",
    "predictions[2] = \"I-ORG\" # Changing the prediction to test our eval\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defines the evaluation function for evaluating a trained NER model and loading a model with label mapping:\n",
    "\n",
    "**1. Evaluation Function (`compute_metrics`):**\n",
    "  - Takes model predictions (`logits`) and ground truth labels.\n",
    "  - Converts logits to predicted labels (argmax).\n",
    "  - Removes ignored labels (`-100`) from both predictions and ground truth.\n",
    "  - Uses an external metric library (`metric`) to compute precision, recall, F1, and accuracy.\n",
    "  - Returns a dictionary containing these metrics.\n",
    "\n",
    "**2. Label Mapping:**\n",
    "  - Creates dictionaries `id2label` and `label2id` to map between label IDs and their actual names.\n",
    "  - `id2label`: Maps numerical label ID to its corresponding NER tag name.\n",
    "  - `label2id`: Maps the NER tag name to its corresponding numerical ID.\n",
    "\n",
    "**3. Model Loading:**\n",
    "  - Loads the Arabic BERT-based model (`AutoModelForTokenClassification`) from the specified checkpoint (`model_checkpoint`).\n",
    "  - Provides the label mapping dictionaries (`id2label` and `label2id`) during model loading.\n",
    "\n",
    "This code prepares for model evaluation by defining how to calculate performance metrics and ensures the model can interpret labels correctly. You'll likely need to specify the `metric` library used for evaluation (e.g., `seqeval` or `evaluate`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trains and evaluates a NER model for Arabic text using the  Transformers library:\n",
    "\n",
    "**1. Training Arguments:**\n",
    "  - Defines training hyperparameters using `TrainingArguments`:\n",
    "      - `output_dir` : set for model saving.\n",
    "      - `evaluation_strategy`: Evaluates the model after each epoch.\n",
    "      - `save_strategy`: Saves the model checkpoint after each epoch.\n",
    "      - `learning_rate`: Sets the learning rate to 2e-5.\n",
    "      - `num_train_epochs`: Trains for 3 epochs.\n",
    "      - `weight_decay`: Applies weight decay (0.01).\n",
    "      - `push_to_hub`: Disables pushing the model to the Hugging Face Hub (set to `False`).\n",
    "\n",
    "**2. Trainer Setup:**\n",
    "  - Creates a `Trainer` object to manage the training and evaluation process.\n",
    "  - Provides the following arguments to the trainer:\n",
    "      - `model`: The loaded Arabic BERT-based model.\n",
    "      - `args`: The defined training arguments.\n",
    "      - `train_dataset`: The preprocessed training dataset (`tokenized_datasets[\"train\"]`).\n",
    "      - `eval_dataset`: The preprocessed validation dataset (`tokenized_datasets[\"validation\"]`).\n",
    "      - `data_collator`: The data collator for batching and padding (`data_collator`).\n",
    "      - `compute_metrics`: The function to calculate evaluation metrics (`compute_metrics`).\n",
    "      - `tokenizer`: The Arabic BERT tokenizer (`tokenizer`).\n",
    "\n",
    "**3. Training and Evaluation:**\n",
    "  - Calls `trainer.train()` to train the model on the provided training dataset.\n",
    "  - Calls `trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])` to evaluate the model's performance on the test dataset after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import accelerate\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"aubmindlab/bert-base-arabert\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3846' max='3846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3846/3846 08:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.219500</td>\n",
       "      <td>0.186373</td>\n",
       "      <td>0.813286</td>\n",
       "      <td>0.835622</td>\n",
       "      <td>0.824303</td>\n",
       "      <td>0.944874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.121700</td>\n",
       "      <td>0.163757</td>\n",
       "      <td>0.845081</td>\n",
       "      <td>0.869144</td>\n",
       "      <td>0.856944</td>\n",
       "      <td>0.952393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.085200</td>\n",
       "      <td>0.172738</td>\n",
       "      <td>0.854658</td>\n",
       "      <td>0.863288</td>\n",
       "      <td>0.858951</td>\n",
       "      <td>0.954198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3846, training_loss=0.16325811117314076, metrics={'train_runtime': 484.4064, 'train_samples_per_second': 63.48, 'train_steps_per_second': 7.94, 'total_flos': 619559527872024.0, 'train_loss': 0.16325811117314076, 'epoch': 3.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='322' max='322' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [322/322 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2298029363155365,\n",
       " 'eval_precision': 0.8308346839546191,\n",
       " 'eval_recall': 0.8525987525987526,\n",
       " 'eval_f1': 0.8415760311922841,\n",
       " 'eval_accuracy': 0.9438562171414598,\n",
       " 'eval_runtime': 6.894,\n",
       " 'eval_samples_per_second': 373.077,\n",
       " 'eval_steps_per_second': 46.707,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and making a predictions with our trained model with HuggingFace Pipline:\n",
    "\n",
    "**1. Model Loading:**\n",
    "- Loads our pre-trained NER model from the specified checkpoint (`model_checkpoint`).\n",
    "- Uses the `pipeline` function from Transformers to create a ready-to-use NER pipeline.\n",
    "- Sets `aggregation_strategy` to \"simple\" (default), which means the predicted entity label for a token is the most likely one across all possible labels.\n",
    "\n",
    "**2. NER Prediction:**\n",
    "- Stores the input Arabic sentence in the variable `comp_sent`.\n",
    "- Calls the loaded NER pipeline (`token_classifier`) on the input sentence (`comp_sent`).\n",
    "- Saves the resulting predictions in the variable `preds`.\n",
    "\n",
    "**3. Output:**\n",
    "- Iterates through the predictions (`preds`):\n",
    "    - For each prediction:\n",
    "        - Prints the word (`i['word']`).\n",
    "        - Prints the predicted entity label (`i['entity_group']`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"aubmindlab/bert-base-arabert/checkpoint-3846/\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_sent = \"اتهمت الصين يوم الخميس تايبيه بإفساد الأجواء لاستئناف المحادثات عبر مضيق تايوان بزيارة نائب الرئيس التايواني ليان تشان إلى أوكرانيا هذا الأسبوع والتي أثارت غضب بكين\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = token_classifier(comp_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the word الصين\n",
      " is labeled as LOC\n",
      "the word تايبيه\n",
      " is labeled as LOC\n",
      "the word تايوان\n",
      " is labeled as LOC\n",
      "the word ليان تشان\n",
      " is labeled as PER\n",
      "the word أوكرانيا\n",
      " is labeled as LOC\n",
      "the word بكين\n",
      " is labeled as LOC\n"
     ]
    }
   ],
   "source": [
    "for i in preds:\n",
    "    print(f\"the word {i['word']}\\n is labeled as {i['entity_group']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
